"""
ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - Alpha Vantage APIã‹ã‚‰USD/JPYãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import os
import sys
import json
import time
from datetime import datetime, timedelta
import logging
import yaml
import requests
import pandas as pd
from pathlib import Path
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ãƒ­ãƒ¼ãƒ‰
load_dotenv(os.path.join(os.path.dirname(__file__), '..', '.env'))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataFetcher:
    """Alpha Vantage APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path='config.yaml'):
        """åˆæœŸåŒ–"""
        self.config = self._load_config(config_path)
        self.api_key = os.getenv('ALPHA_VANTAGE_KEY', 'demo')
        self.base_url = self.config['api']['base_url']
        self.timeout = self.config['api']['timeout']
        self.raw_data_path = Path(self.config['data']['raw_data_path'])
        self.raw_data_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"DataFetcher initialized with API key: {self.api_key[:10]}...")
    
    @staticmethod
    def _load_config(config_path):
        """YAMLã‚³ãƒ³ãƒ•ã‚£ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def fetch_intraday(self, symbol='USDJPY', interval='1min'):
        """
        Alpha Vantage Intraday APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            symbol (str): é€šè²¨ãƒšã‚¢ (e.g., 'USDJPY')
            interval (str): æ™‚é–“è¶³ ('1min', '5min', '15min' ãªã©)
        
        Returns:
            pd.DataFrame: OHLCV ãƒ‡ãƒ¼ã‚¿
        """
        logger.info(f"Fetching {symbol} with interval {interval}...")
        
        params = {
            'function': 'FX_INTRADAY',
            'from_symbol': symbol[:3],  # USD
            'to_symbol': symbol[3:],    # JPY
            'interval': interval,
            'apikey': self.api_key,
            'outputsize': 'full'  # å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
        }
        
        try:
            response = requests.get(
                self.base_url,
                params=params,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            data = response.json()
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if 'Error Message' in data:
                logger.error(f"API Error: {data['Error Message']}")
                return None
            
            if 'Note' in data:
                logger.warning(f"API Note: {data['Note']} (Rate limit)")
                return None
            
            # ã‚¿ã‚¤ãƒ ã‚·ãƒªãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if 'Time Series (1min)' not in data and 'Time Series FX (Intraday)' not in data:
                logger.error("No time series data found in response")
                logger.debug(f"Response keys: {data.keys()}")
                return None
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
            ts_key = list(data.keys())[1]  # 'Time Series (1min)' ãªã©
            ts_data = data[ts_key]
            
            # DataFrameã«å¤‰æ›
            df = pd.DataFrame.from_dict(ts_data, orient='index')
            df.index = pd.to_datetime(df.index)
            df = df.sort_index()
            
            # ã‚«ãƒ©ãƒ åã‚’æ¨™æº–åŒ–
            df.columns = [
                'open', 'high', 'low', 'close'
            ]
            
            # æ•°å€¤ã«å¤‰æ›
            for col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # NaNã‚’å‰Šé™¤
            df = df.dropna()
            
            logger.info(f"âœ… Fetched {len(df)} records for {symbol}")
            return df
        
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Network error: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Error fetching data: {e}")
            return None
    
    def fetch_demo_data(self, symbol='USDJPY', interval='1min'):
        """
        ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆAIãƒ†ã‚¹ãƒˆç”¨ï¼‰
        å®Ÿéš›ã®ApiãŒä½¿ãˆãªã„ç’°å¢ƒã§ã®æ¤œè¨¼ç”¨
        """
        logger.info(f"Generating demo data for {symbol}...")
        
        # éå»200æ™‚é–“åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        dates = pd.date_range(
            end=datetime.now(),
            periods=200,
            freq='1min'
        )
        
        # ä»®æƒ³çš„ãªãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ä¾¡æ ¼
        import numpy as np
        np.random.seed(42)
        
        base_price = 145.0
        log_returns = np.random.normal(0.0001, 0.002, len(dates))
        prices = base_price * np.exp(np.cumsum(log_returns))
        
        df = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.01, len(dates)),
            'high': prices + np.random.uniform(0, 0.05, len(dates)),
            'low': prices - np.random.uniform(0, 0.05, len(dates)),
            'close': prices,
        }, index=dates)
        
        logger.info(f"âœ… Generated {len(df)} demo records")
        return df
    
    def save_data(self, df, symbol='USDJPY'):
        """ãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜"""
        if df is None or len(df) == 0:
            logger.warning("No data to save")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = self.raw_data_path / f"{symbol}_{timestamp}.csv"
        
        df.to_csv(filename)
        logger.info(f"ğŸ’¾ Saved data to {filename}")
        return filename
    
    def get_latest_data(self, symbol='USDJPY', days=7):
        """
        æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³ç”¨ï¼‰
        
        Args:
            symbol (str): é€šè²¨ãƒšã‚¢
            days (int): éå»Næ—¥åˆ†ã‚’è¿”ã™
        
        Returns:
            pd.DataFrame: ãƒ‡ãƒ¼ã‚¿
        """
        # raw_data_pathã‹ã‚‰æœ€æ–°ã®CSVã‚’æ¢ã™
        csv_files = sorted(self.raw_data_path.glob(f"{symbol}_*.csv"))
        
        if not csv_files:
            logger.warning(f"No CSV files found for {symbol}")
            return None
        
        latest_file = csv_files[-1]
        logger.info(f"Loading from {latest_file}")
        
        df = pd.read_csv(latest_file, index_col=0, parse_dates=True)
        
        # éå»Nãƒ‡ãƒ¼åˆ†ã‚’è¿”ã™
        cutoff_date = datetime.now() - timedelta(days=days)
        df = df[df.index >= cutoff_date]
        
        logger.info(f"âœ… Loaded {len(df)} records from {latest_file}")
        return df


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import sys
    
    logger.info("=" * 50)
    logger.info("ğŸ”„ USD/JPY Data Fetching Pipeline")
    logger.info("=" * 50)
    
    fetcher = DataFetcher('config.yaml')
    
    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— (API KeyãŒ'demo'ã®å ´åˆ)
    if fetcher.api_key == 'demo':
        logger.info("Using demo data (API key is 'demo')")
        df = fetcher.fetch_demo_data('USDJPY', '1min')
    else:
        # å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = fetcher.fetch_intraday('USDJPY', '1min')
    
    if df is not None:
        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        fetcher.save_data(df, 'USDJPY')
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        logger.info("\nğŸ“Š Data Statistics:")
        logger.info(f"  Rows: {len(df)}")
        logger.info(f"  Date range: {df.index[0]} to {df.index[-1]}")
        logger.info(f"  Close price range: {df['close'].min():.4f} ~ {df['close'].max():.4f}")
        
        return df
    else:
        logger.error("âŒ Failed to fetch data")
        return None


if __name__ == '__main__':
    main()
